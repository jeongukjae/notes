---
layout: post
title: "2019 머신러닝 스터디잼 중급반"
tags:
  - studyjam
---

[머신러닝 스터디잼 중급반](https://sites.google.com/view/ml-studyjam2/?fbclid=IwAR2IpboQjpHLIG8PESFpoP_K2DZwXJcGqk48nabWdRMrz720aS8gGyKtGoE)!! 도 하게 되어서 간략하게 정리해보는 포스트를 작성하려 한다. 이번에도  coursera와 qwiklab을 이용하는 것을 보인다. cousera는 [Launching Machine Learning](https://www.coursera.org/learn/launching-machine-learning)을 수강하는 것이 목표이고, qwiklab은 [Classify Images of Clouds in the Cloud with AutoML Vision](https://www.qwiklabs.com/focuses/1779?catalog_rank=%7B%22rank%22%3A1%2C%22num_filters%22%3A0%2C%22has_search%22%3Atrue%7D&parent=catalog&search_id=2274219)을 수강하는 것이 목표이다.

아는 내용은 키워드만 적고 넘어가고 헷갈리는 부분 & 모르는 내용만 자세히 정리한다.

강의에서는 아래와 같은 내용을 배운다고 한다.

* 딥러닝이 왜 그렇게 이슈가 되었는가
* loss function과 performance metric을 이용해 최적화하기
* ml에서 나온 문제들을 쉽게 풀어보기 (어떻게 번역해야할지 모르겠다. 원래의 말은 mitigate common problems that arise in machine learning이다)
* test dataset을 모으고, 훈련하고 evaluation까지 해보기

이런 내용을 아래와 같은 모듈로 나누어서 강의를 한다.

1. Practical ML
2. Optimization
3. Generalization and Sampling
4. Summary

## Practical ML

이 모듈에서는 ML의 주요한 문제들을 살펴보고 왜 그토록 이슈가 되었는지 살펴본다.

아래와 같은 키워드를 다룬다.

* Supervised Machine Learning vs Unsupervised
* Two types of supervised machine learning
  * classification vs regression
  * classification은 분류 문제
  * regression은 값을 예측하는 문제

### Short History of ML

#### Linear Regression

첫번째는 Linear Regression에 관한 간단한 역사를 다룬다. Linear Regression은 행성의 움직임과 같은 자연현상에 대한 이해를 위해 발전했다. linear regression은 input feature로 들어오는 값들에 각각 weight를 곱해서 결과(prediction)를 뽑아낸다. 즉 아래와 같은 식을 이용한다.

$$ \hat y  = X w$$

머신러닝에서도 많이 사용되는 식이다. 근데, weight를 선택할 방법이 없어 적당한 값을 찾기 위해 loss function을 만들었다. 여기서 설명하는 loss function은 mean squared error였다. 하지만, 그 loss function에서 바로 weight를 뽑아내기에도 어려움이 따르니, optimizing 하는 다른 방법을 생각한 것이 Gradient Descent이다.

#### Perceptron

1940년대에 Frank Rosenblatt이 인간의 뇌를 본딴 모델이 간단한 함수를 학습할 수 있다는 것을 알게 되었다. single layer를 가지는 perceptron이 lienar한 함수를 익힐 수 있다는 것을 보여주었다. (linear classifier) 하지만, XOR과 같은 lienar 하지 않은 것은 학습하지 못한다.

#### Neural Network

XOR같은 것을 학습하지 못하니까 그래서 여러 단의 layer를 쌓기 시작했다. 그 사이에 activation function도 넣고. 특히 ReLU까지 적용되기 시작하고 나서는 굉장히 빠르게 학습도 가능해졌다.

#### Deicsion Trees

decision tree는 piecewise linear dicision boundary를 학습하는데, 이는 학습하기도 쉽고, 사람들이 그 결과를 이해하기도 쉽다. classification, regression에 둘다 쓰일 수 있다. 이름에 맞게 각각의 node는 하나의 feature에 대한 lienar classifier로 이루어진다.

#### Kernel Methods

SVM은 혁신적이었다..! 하지만, linear하게 decision boundary를 결정했으므로, non linear한 것에 대해서는 적절하게 나눌 수가 없었다. 그래서 kernel transform을 적용하기 시작했다. 그 방법을 적용한 SVM이 kernelized svm. NN에서는 layer에 더 많은 neuron을 넣어주는 것이 higher dimension으로 mapping해주는 요소로 생각하면 된다.

#### Random Forests

더 많은 classifier, regressor를 사용해서 ensemble하는 것이 더 좋은 성능을 내므로, 정말 그렇게 한 것이다. Tree -> Forest로 생각하면..? 하나의 tree가 모든 것을 기억할 수 없고, 독립적으로 고려해야할 요소가 있을 수 있으니 이렇게 하는 것 같다. random holdout을 사용하는 k-fold validation와도 비슷하다고 한다.

#### Modern Neural Networks

하드웨어가 좋아지고, NN에 대한 많은 좋은 방법이 제안되면서 DNN이 정말 유명해지기 시작했다. 이 영상은 특정 모델을 두고만 말해서 좀 많이 건너 뛰었다 ㅠㅠ

## Optimization

아래와 같은 것들을 배운다고 한다.

* Quantify model performance using loss functions
* Use loss functions as the basis for an algorithm called gradient descent
* Optimize gradient descent to be as efficient as possible
* Use performance metrics to make business decisions

### Defining ML Models

ML 모델은 parameter와 hyper parameter로 이루어진 함수라고 생각할 수 있다. 현재 데이터에 대해서만 decision boundary를 긋는 것이 아니라 새로 나타날 데이터에 대해 decision boundary를 긋는 것도 중요하다.

### Introducing Loss Function

얼마나 우리가 원하는 답으로부터 멀어져있는지 알수있는 함수이다. RMSE같은 loss function을 적용해볼 수 있다.

RMSE는 잘 동작할 때가 많지만, classification용으로는 잘 동작하지 않는다. 그래서 Classification 문제를 해결하기 위해 Cross Entropy Loss을 적용할 때가 있다.

생각해보니 위에 것들을 필기한 이후로 내용 체크를 안하고 끝내버렸다..\.. ㅠㅠㅠㅠㅠㅠㅠㅠㅠ
