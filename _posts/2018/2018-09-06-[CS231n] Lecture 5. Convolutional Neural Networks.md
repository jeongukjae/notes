---
layout: post
title: "CS231n Lecture 5. Convolutional Neural Networks"
tags:
  - cs231n
---

* 강의 영상 : [링크](https://www.youtube.com/watch?v=bNb2fEVKeEo)
* 강의 슬라이드 : [링크](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf)

대충 정리합니다.

## _

이제부터 Fully Connected Lyaer에서 CNN 관련된 내용으로 넘어감.

## History

* 1957년에 Backprop 개념은 없는 Mark I Percentron이라는 기계도 있었음
* 1960년에 Widrow and Hoft라는 기계가 만들어졌는데, multilayer를 쓰기 시작함 (layer를 쌓기 시작함)
* 1986년에 backprop이 유명해지기 시작
* 2006년에 Deep learning이 다시 떠오르기 시작(reinvigorate)

고양이의 뇌를 이용해 연구하기 시작. Visual Stimuli가 세포들의 heirachical orgranization을 통해 전파됨. Simple Cell - Complex Cell 등등을 연결한 Sandwich architecture라는 아키텍쳐도 생김.

1998년에 Gradient based learning[^lecun] 나옴

그 후에 ImageNet 대회 등도 하고 AlexNet이라는 아키텍쳐도 나오고 유명한 네트워크 아키텍쳐들이 많대요.

## Today

지금 배울 ConvNet이 유명하다네요.

아래와 같은 분야에 쓰인답니다.

* classification
* retreival
* detection
* segmentation
* face/pose recognition
* deep dream (image to image translation 같은 느낌인 것 같네요. 그 당시에 딥러닝 커뮤니티에 엄청 이슈였던 것은 기억합니다)

## CNN

### FC Layer와의 비교

FC Layer는 모든 neuron을 펼쳐서 neural network를 만드는데 비해, CNN은 Spartial Structure를 유지하기 위해 filter를 사용한다. 그냥 신호처리? 같은 곳에서 사용하는 Convolution을 생각하면 된다. 슬라이드에 적절한 그림이 있다.

### CNN

32x32x3의 구조를 가지는 입력에 5x5x3의 구조를 가지는 filter를 적용할 때 (depth, 마지막 사이즈는 같아야 한다. 여기서는 3) 총 75-d (5*5*3) dot product + bias 같은 방식으로 연산한다. b는 단순 스칼라값.

$$w^Tx + b$$

그렇게 해서 모인 하나하나의 숫자들을 nxnx1 사이즈로 모은 것이 하나의 activation map. 원래의 Convolution은 flipping을 하지만 이 수업에서는 심각하게 고려하지 않는다고 한다. 32x32x3을 5x5x3의 filter로 stride 1값으로 하나의 activation map을 만들면 size of activation map은 28x28x1이 된다.

물론 하나의 filter를 사용하진 않는다. 6개의 필터를 쓰면 activation map은 28x28x6이 될 것이다. 하나의 filter가 하나의 activation map을 생성하고 이를 쌓아서 출력값이 된다.

여기서 ConvNet은 sequence of convolutional layers라고 한다.

이 layer가 convolutional 하다고 불리는 이유는 두 신호의 convolution과 관련이 있기 때문이다. 근데 물론 지금 당장 크게 중요하진 않다.

$$ f\langle x, y\rangle * g\langle x, y\rangle = \sum^\infty_{n_1=-\infty}\sum^\infty_{n_2=-\infty} f\langle n_1, n_2 \rangle g\langle x - n_1, y - n_2 \rangle$$

### A closer look at spatial dimensions

Stride를 쓸 때는 맞지 않는 Stride는 쓰지마라. 예를 들어 7x7x3가 input이고, filter 가 3x3x3인데, stride가 3이면, 마지막에도 적절하게 들어맞지 않는다. (그냥 기억안나면 강의 슬라이드 참고해자)

실전에서는 zero-pad를 자주 쓴다고 한다. 3x3xn의 filter를 쓸 때 상하좌우로 1px씩 줄어들어서 zero-pad를 해서 1px씩 주는걸 예방한다. 네트워크를 거칠수록 계속해서 줄어들어서 정보들을 잃을 가능성이 크다.

### the number of parameters

10개의 5*5*3 filter를 사용하면 몇개의 파라미터가 필요할까?

하나의 필터당 5 * 5 * 3 + 1개의 파라미터가 필요하다. (+1은 bias이다.) 그리고 76개가 10개만큼 필요하니 760개의 파라미터가 필요하다.

보통 필터는 $$2^n$$개를 쓴다고 한다.

1x1xn filter도 가끔 있다고 한다.

## Pooling layer

makes the representations smaller and more manageable이라고 설명한다. downsampling이라 보면 된다. operates over each activation map independently라고 한다.

max pooling는 2x2 filter를 사용할 때는 4칸의 neuron 중 max값만을 취해서 stride 2만큼 옮기면서 계산값을 낸다. 그래서 1/4 사이즈로 줄어든다.

average pooling보다 많이 쓴다는데 그 이유는 각 neuron의 값이 얼마나 fire되었냐를 나타내기 좋아서 max가 좋다고 한다.

보통 stride를 2로, filter width,height를 2 혹은 3으로 쓴다고 한다.

## at the end

ConvNet끝에 fully connected layer를 연결한다고 한다. 모든 값을 늘려서 1xn 벡터로 만든다음 일반적인 neural network와 연결한다.

ConvNetJS demo를 참고해봐라고 한다. url은 [http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html) 여기. CIFAR-10 데이터셋을 썼다는데 나도 나중에 해봐야겠다.

[^lecun]: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf 혁신적인 논문이었다고 합니다.
