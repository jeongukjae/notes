---
layout: post
title: CS330 Lecture 1 Introduction & Overview
tags:
  - cs330
---

얼마전 페이스북에서 Multi-task and Meta Learning 이라는 제목을 달고있는 Stanford CS330을 달고 있는 강의를 보아서 들어보기로 했다. 14개 정도의 강의라 배속으로 적당히 빨리 들어봐야겠다.

* 강의 사이트 <http://cs330.stanford.edu/>
* 강의 비디오 <https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5>

시간이 많이 흘러서(강의 비디오는 2019년 가을) 내용이 많이 바뀌겠지만, 해당 내용은 발표 슬라이드로 어떻게 채워봐야겠다.

* [1강 pdf](https://cs330.stanford.edu/slides/cs330_intro.pdf)

---

* 하나의 environment에서 하나의 task를 배우는데 여기에는 많은 supervision과 guidance가 필요하다. 이건 강화학습이나 로보틱스, speech recognition등등 많은 분야에 적용되는 이야기
* deep multi-task, meta-learning을 신경써야 하는 이유
  * 크고 다양한 데이터를 이용하고 큰 모델을 사용한다면 머신러닝 모델이 잘 generalize하는 것은 기존에 잘 알려져 있다.
    * 하지만 large dataset을 이용할 수 없다면 이야기는 달라진다. (medical imaging이나 robotics, medicine, recommendations 등등을 생각해보자) 각각의 태스크를 학습하기 힘들어진다.
    * 또는 long tail dataset에 대해서 학습한다고 생각해보자. 일반적인 supervised learning만으로는 학습하기 힘들다.
    * 아니면 빠르게 새로운 태스크에 대응해야 할 때는? -> 사람이라면 기존의 지식을 기반으로 빠르게 학습이 가능하다.
  * 위와 같은 상황에서 multi-task learning이나 meta learning이 필요하다.
* 여기서 multi-task/meta learning을 사용하기 위해서는 여러 태스크가 같은 structure를 공유해야 한다.
  * 만약 관계가 없어보이더라도 한국어 데이터라면 최소한 한국어의 룰에 대한 부분은 공유한다는 점을 생각해보고, 언어는 비슷한 목적을 위해 만들어졌다는 점을 생각해보면 임의의 태스크보다는 훨씬 관계있어 보인다고 한다.
* informal하게 강의 주제를 정의해보면
  * multitask learning problem: learn all of tasks more quickly or more proficiently than learning them independently.
  * meta learning problem: given data/experience on previous tasks, learn a new task more quickly and/or more proficiently.
* 그럼 domain adaptation과 무엇이 다를까.
  * domain adaptation이 배우는 것은 새로운 학습 데이터가 이전 학습 데이터에서의 out of distribution이라는 점 정도
* 근데 multi task learning은 single task learning으로 볼 수 있지 않나요?
  * dataset의 합집합으로 보고 loss를 각각 태스크의 loss의 합으로 보면 그렇다.
  * 근데 해당 방법은 multi task의 하나의 방법이지 전부가 아니고, 서로 다른 태스크라는 정보로 더 나은 성능을 위해 시도해볼 수 있는 것들이 있다.
